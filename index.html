<!doctype html>
<html lang="en">

	<head>
		<meta charset="utf-8">

		<title>Differentially Private Gaussian Processes</title>

		<meta name="description" content="Differentially Private Gaussian Processes">
		<meta name="author" content="Mike Smith">

		<meta name="apple-mobile-web-app-capable" content="yes">
		<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">

		<link rel="stylesheet" href="css/reveal.css">
		<link rel="stylesheet" href="css/theme/black.css" id="theme">

		<!-- Code syntax highlighting -->
		<link rel="stylesheet" href="lib/css/zenburn.css">

		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>

		<!--[if lt IE 9]>
		<script src="lib/js/html5shiv.js"></script>
		<![endif]-->
	</head>

	<body>

<div class="reveal">

<!-- Any section element inside of this container is displayed as a slide -->
<div class="slides" data-background="assets/pres_bg.png">			

<section data-background="assets/pres_bg_bb_inv.png" style="color:black;">
	<h3 style="color:black;">Differentially Private<br/> Gaussian Processes</h3>
	<p>
		<small>Presented by Mike Smith</small><br/>
        <small><a href="mailto:m.t.smith@sheffield.ac.uk">m.t.smith@sheffield.ac.uk</a></small>        
	</p>
	<p>
        <small><br/><br/>Michael Thomas Smith,  Max Zwiessele, Mauricio A Alvarez Lopez, Neil D. Lawrence</small>	
    </p>
</section>

<section data-background="assets/pres_bg_bb_inv.png" style="color:black;">
	<h3 style="color:black;">Differential Privacy for GPs</h3>
	<p>We have a dataset in which the inputs, $X$, are <b>public</b>. The outputs, $\mathbf{y}$, we want to keep <b>private</b>.</p>
    <img src="assets/kung_pseudo_pert.png" width="75%" style="margin:0px; border:none;" />
    <p style="font-size:small;"><b>Data consists of the heights and weights of 287 women from a census of the !Kung</b></p>
</section>


<section data-background="assets/pres_bg_bb_inv.png" style="color:black;">
	<h3 style="color:black;">Vectors and Functions</h3>
	<p>Hall et al. (2013) showed that one can ensure that a version
of $f$, function $\tilde{f}$ is $(\varepsilon, \delta)$-differentially private by adding a scaled sample from a GP prior.</p>
<img src="assets/hall1.png" width="40%" style="margin:0px; border:none;" />
<p><small>3 pages of maths ahead!</small></p>
</section>

<section data-background="assets/pres_bg_bb_inv.png" style="color:black;">
	<h3 style="color:black;">Applied to Gaussian Processes</h3>
	<p>We applied this method to the GP posterior.</p>
	<p class="fragment">The covariance of the posterior only depends on the inputs, $X$. So we can compute this without applying DP.</p>
	<p class="fragment">The mean function, $f_D(\mathbf{x_*})$, does depend on $\mathbf{y}$.
	$f_D(\mathbf{x_*}) = \mathbf{k}_*^\top K^{-1} \mathbf{y}$</p>
	<p class="fragment">We are interested in finding $|| f_D(\mathbf{x_*}) - f_{D^\prime}(\mathbf{x_*}) ||_H^2$</p>
	<p class="fragment">...how much the mean function (in RKHS) can change due to a change in $\mathbf{y}$.</p>
</section>

<section data-background="assets/pres_bg_bb_inv.png" style="color:black;">
	<h3 style="color:black;">Applied to Gaussian Processes</h3>
	<p>Using the representer theorem, we can write $|| f_D(\mathbf{x_*}) - f_{D^\prime}(\mathbf{x_*}) ||_H^2$<br/> as:</p>
	<p>$\Big|\Big|\sum_{i=1}^n k(\mathbf{x_*},\mathbf{x}_i) \left(\alpha_i - \alpha^\prime_i\right)\Big|\Big|_H^2$</p>
	<p>where $\mathbf{\alpha} - \mathbf{\alpha}^\prime = K^{-1} \left(\mathbf{y} - \mathbf{y}^\prime \right)$</p>	
</section>

<section data-background="assets/pres_bg_bb_inv.png" style="color:black;">
    <p>$\Big|\Big|\sum_{i=1}^n k(\mathbf{x_*},\mathbf{x}_i) \left(\alpha_i - \alpha^\prime_i\right)\Big|\Big|_H^2$</p>
	<p>where $\mathbf{\alpha} - \mathbf{\alpha}^\prime = K^{-1} \left(\mathbf{y} - \mathbf{y}^\prime \right)$</p>
	<p class="fragment">We constrain the kernel: $-1\leq k \leq 1$ and we only allow one element of $\mathbf{y}$ and $\mathbf{y}'$ to differ (by at most $d$).</p>
	<p class="fragment">So only one column of $K^{-1}$ will be involved in the change of mean (which we are summing over).</p>
	<p class="fragment">The distance above can then be shown to be no greater than $d\;||K^{-1}||_\infty$	
</section>

<section data-background="assets/pres_bg_bb_inv.png" style="color:black;">
    <h3 style="color:black;">Applied to Gaussian Processes</h3>
    <p>This 'works' in that it allows DP predictions...but to avoid too much noise, the value of $\varepsilon$ is too large (here it is 100)</p>
    <img src="assets/kung_standard_simple.png" width="70%" style="margin:0px; border:none;" />
    <p><small>EQ kernel, $l = 25$ years, $\Delta=100$cm</small></p>
</section>

<section data-background="assets/pres_bg_bb_inv.png" style="color:black;">
    <h3 style="color:black;">Inducing Inputs</h3>
    <p>Using sparse methods (i.e. inducing inputs) can help reduce the sensitivity a little.</p>
    <img src="assets/kung_pseudo_simple.png" width="70%" style="margin:0px; border:none;" />
</section>


<section data-background="assets/pres_bg_bb_inv.png" style="color:black;">
    <h3 style="color:black;">Cloaking</h3>
    <p>So far we've made the whole posterior mean function private...</p>
    <p>...what if we just concentrate on making particular predictions private?</p>
</section>

<section data-background="assets/pres_bg_bb_inv.png" style="color:black;">
    <h3 style="color:black;">Effect of perturbation</h3>
    <p>Previously I mentioned that the noise is sampled from the GP's <b>prior</b>.</p>
    <p>This is not necessarily the most 'efficient' covariance to use.</p>
    <img src="assets/cloak1.png" width="75%" style="margin:0px; border:none;" />
</section>

<section data-background="assets/pres_bg_bb_inv.png" style="color:black;">
    <h3 style="color:black;">Effect of perturbation</h3>
    <img src="assets/cloak2.png" width="75%" style="margin:0px; border:none;" />
</section>

<section data-background="assets/pres_bg_bb_inv.png" style="color:black;">
    <h3 style="color:black;">Cloaking</h3>
    <p>Left: Ideal covariance. Right: actual covariance</p>
    <img src="assets/cloak3.png" width="75%" style="margin:0px; border:none;" />
</section>

<section data-background="assets/pres_bg_bb_inv.png" style="color:black;">
    <h3 style="color:black;">DP Vectors</h3>
    <p>Hall et al. (2013) also presented a bound on vectors.</p>
    <p>Find a bound ($\Delta$) on the scale of the output change, in term of its Mahalanobis distance (wrt the added noise covariance).</p>
    <p>$\sup_{D \sim {D'}} ||M^{-1/2} (\mathbf{y}_* - \mathbf{y}_{*}')||_2 \leq \Delta$</p>
    <p>We use this to scale the noise we add:</p>
    <p>$\frac{\text{c}(\delta)\Delta}{\varepsilon} \mathcal{N}_d(0,M)$</p>
    <p>We get to pick $M$</p>
</section>

<section data-background="assets/pres_bg_bb_inv.png" style="color:black;">
    <h3 style="color:black;">Cloaking</h3>
    <p>Intuitively we want to construct $M$ so that it has greatest covariance in those directions most affected by changes in training points, so that it will be most able to mask those changes.</p>
    <p>The change in posterior mean predictions is,</p>
    <p>$\mathbf{y}_* - \mathbf{y}'_* = K_{*f} K^{-1} (\mathbf{y}-\mathbf{y}')$</p>
    <p>The effect of perturbing each training point on each test point is represented in the cloaking matrix, $C = K_{*f} K^{-1}$
</section>

<section data-background="assets/pres_bg_bb_inv.png" style="color:black;">
    <h3 style="color:black;">Cloaking</h3>
    <p>We assume we are protecting only one training input's change, by at most $d$. </p>
    <p>So $\mathbf{y}-\mathbf{y}'$ will be all zeros except for one element, $i$.<br />So the change in test points will be (at most)</p>
    <p>$\mathbf{y}_*' - \mathbf{y}_* = d C_{:i}$</p>
    <p>We're able to write the earlier bound as,</p>
    <p>$d^2 \sup_{i} \mathbf{c}_i^\top M^{-1} \mathbf{c}_i \leq \Delta$</p>
    <p><small>where $\mathbf{c}_i \triangleq C_{:i}$</small></p>
</section>

<section data-background="assets/pres_bg_bb_inv.png" style="color:black;">
    <h3 style="color:black;">Cloaking</h3>
    <p>Dealing with $d$ elsewhere and setting $\Delta = 1$ (thus $0 \leq \mathbf{c}_i^\top M^{-1} \mathbf{c}_i \leq 1$) and minimise $\log |M|$ <br/>(minimises the partial entropy).</p>
    <p>Using lagrange multipliers and gradient descent, we find</p>
    <p>$M = \sum_i{\lambda_i \mathbf{c}_i \mathbf{c}_i^\top}$</p>
</section>


<section data-background="assets/pres_bg_bb_inv.png" style="color:black;">
    <h3 style="color:black;">Cloaking: Results</h3>
    <p>Noise is now practical, and has interesting features;</p>
    <ul>
    <li>Less noise where data is concentrated</li>
    <li>Least noise far from any data</li>
    <li>Most noise just outside data</li>
    </ul>
    <p></p>
    <img src="assets/kung_cloaking_simple.png" width="80%" style="margin:-10px; border:none;" />
    <p><small>EQ kernel, $l = 25$ years, $\Delta=100$cm, $\varepsilon=1$</small></p>
</section>

<section data-background="assets/pres_bg_bb_inv.png" style="color:black;">
    <p>House prices around London</p>
    <img src="assets/houseprices_bigcirc_15km_0_labels.png" width="60%" style="margin:0px; border:none;" />
</section>

<section data-background="assets/pres_bg_bb_inv.png" style="color:black;">
    <h3 style="color:black;">Citibike</h3>
    <p>Tested on 4d citibike dataset (predicting journey durations from start/finish station locations).
    <p>The method appears to achieve lower noise than binning alternatives (for reasonable $\varepsilon$).</p>
    <img src="assets/citibike_results.png" width="100%" style="margin:0px; border:none;" />
    <small>lengthscale in degrees, values above, journey duration (in seconds)</small>
</section>

<section data-background="assets/pres_bg_bb_inv.png" style="color:black;">
    <h3 style="color:black;">Summary</h3>
    <p>We have developed an improved method for performing differentially private regression.</p>
    <p>Unlike binning this can scale to increased numbers of dimensions</p>
    <h3 style="color:black;">Future work</h3>
    <p>Multiple outputs</p>
    <p>GP classification</p>
    <p>DP Optimising hyperparameters</p>
    <p>Making the inputs private</p>
</section>

<section data-background="assets/pres_bg_bb_inv.png" style="color:black;">
<small>
<p align="left">
<span style='margin-left:-50px;'><b>The go-to book on differential privacy, by Dwork and Roth;</b><br/></span>
Dwork, Cynthia, and Aaron Roth. "The algorithmic foundations of differential privacy." Theoretical Computer Science 9.3-4 (2013): 211-407.
<a href="https://www.cis.upenn.edu/~aaroth/Papers/privacybook.pdf">link</a>
</p>
<p align="left">
<span style='margin-left:-50px;'><b>I found this paper allowed me to start applying DP to GP;</b><br/></span>
Hall, Rob, Alessandro Rinaldo, and Larry Wasserman. "Differential privacy for functions and functional data." The Journal of Machine Learning Research 14.1 (2013): 703-727.
<a href="http://www.stat.cmu.edu/~arinaldo/papers/hall13a.pdf">link</a>
</p>
<p align="left">
<span style='margin-left:-50px;'><b>Data set of !Kung anthropometric measurements;</b><br/></span>
Howell, N. Data from a partial census of the !kung san, dobe. 1967-1969. <a href="https://public.tableau.com/profile/john.marriott#!/vizhome/kung-san/Attributes">link</a>, 1967.</p>
<br/>
<p align="left">
    <img align="right" src="assets/kung_cloaking_simple.png" width="75%" style="margin:0px; border:none;" />
<span style='margin-left:-50px;'><b>Thanks</b><br/></span>
Funders: EPSRC<br/>
</p>

<p>
<p>
<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
</p>
</small>
</section>


</div>

</div>

<script src="lib/js/head.min.js"></script>
<script src="js/reveal.js"></script>

<script>

	// Full list of configuration options available at:
	// https://github.com/hakimel/reveal.js#configuration
	Reveal.initialize({
		controls: true,
		progress: true,
		history: true,
		center: true,

        math: {
            mathjax: 'https://cdn.mathjax.org/mathjax/latest/MathJax.js',
            config: 'TeX-AMS_HTML-full'  // See http://docs.mathjax.org/en/latest/config-files.html
        },

		transition: 'slide', // none/fade/slide/convex/concave/zoom

		// Optional reveal.js plugins
		dependencies: [
			{ src: 'lib/js/classList.js', condition: function() { return !document.body.classList; } },
			{ src: 'plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
			{ src: 'plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
			{ src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
			{ src: 'plugin/zoom-js/zoom.js', async: true },
			{ src: 'plugin/notes/notes.js', async: true },
            { src: 'plugin/math/math.js', async: true }
		]
	});

</script>

</body>
</html>
