<!doctype html>
<html lang="en">

	<head>
		<meta charset="utf-8">

		<title>Differential Privacy</title>

		<meta name="description" content="Intro and motivation for Differential Privacy">
		<meta name="author" content="Mike Smith">

		<meta name="apple-mobile-web-app-capable" content="yes">
		<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">

		<link rel="stylesheet" href="css/reveal.css">
		<link rel="stylesheet" href="css/theme/black.css" id="theme">

		<!-- Code syntax highlighting -->
		<link rel="stylesheet" href="lib/css/zenburn.css">

		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>

		<!--[if lt IE 9]>
		<script src="lib/js/html5shiv.js"></script>
		<![endif]-->
	</head>

	<body>

		<div class="reveal">

			<!-- Any section element inside of this container is displayed as a slide -->
			<div class="slides" data-background="assets/pres_bg.png">
                <section data-background="assets/pres_bg.png">
                </section>
				<section data-background="assets/pres_bg.png">
                    <h3>Explaining</h3>
					<h1>Differential Privacy</h1>
					<p>
						<small>Presented by Mike Smith</small><br/>
                        <small><a href="mailto:mike@michaeltsmith.org.uk">mike@michaeltsmith.org.uk</a></small>
					</p>
				</section>

				<section>
					<h2>Quick Note!</h2>
					<p>
					    I'm presenting other people's work today.</p><p> See <a href="https://www.cis.upenn.edu/~aaroth/Papers/privacybook.pdf">The Algorithmic Foundations of Differential Privacy</a> by Dwork and Roth for a rigorous introduction to the framework.
					</p>
				</section>

				<section data-background="assets/pres_bg.png">
					<h2>Anonymity Gone Wrong</h2>
					<p>
						In the mid-1990s the Massachusetts Group Insurance Commission released 'anonymised' health records for state employees.

					</p>
                    <img src="assets/mutual.jpg" width="50%" />
				</section>

                <section data-background="assets/pres_bg.png">
					<h2>Anonymity Gone Wrong</h2>
					<p>
                        <img src="assets/weld.jpg" width="50%" align="right" style="margin:50px;" />
                        "William Weld, then Governor of Massachusetts, assured the public that GIC had protected patient privacy by deleting identifiers."<br/> <small><a href="https://epic.org/privacy/reidentification/ohm_article.pdf">Broken Promises of Privacy, Paul Ohm</a></small>
					</p>

				</section>
                <section data-background="assets/pres_bg.png">
					<h2>Anonymity Gone Wrong</h2>
					<p>
                        The data was 'anonymised' by removing names. Other identifying columns remained.
                        <img src="assets/example.png" style="margin:50px;" />
					</p>

				</section>

               <section data-background="assets/pres_bg.png">
					<h2>Anonymity Gone Wrong</h2>
					<p>
                        <img src="assets/sweeney.jpg" align="right" style="margin:50px;" />
                        Latanya Sweeney used the voter rolls (which contain the <b>name</b>, <b>address</b>, <b>ZIP code</b>, <b>date of birth</b> and <b>sex</b> of every voter) to find Governor Weld's medical records...</p>
                    <p><span class="fragment">...which she posted to him.
					</p>


				</section>


               <section data-background="assets/pres_bg.png">
					<h2>Anonymity Gone Wrong</h2>
                    <p>This is known as a <b>linkage attack</b>:</p>
                    <p>An attack that uses <b>auxiliary information</b> to compromise privacy in a database.</p>
				</section>
                <section>
					<h2>Quick note!</h2>
					<p>
					    Although a convenient story about privacy, how many people were really at risk from re-identification in this database is <span style="white-space:nowrap;"><a href="http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2076397">still</a> <a href="http://healthaffairs.org/blog/2012/08/10/the-debate-over-re-identification-of-health-information-what-do-we-risk/" style="">being</a> <a href="http://randomwalker.info/publications/no-silver-bullet-de-identification.pdf">debated</a></span>.
					</p>

				</section>


               <section>
					<h2>Another Note:</h2>
                    <p>Because of swapping (which we'll get to) the following is a fictitious example.</p>
               </section>


        <section data-background="assets/pres_bg.png">
					<h2>The 2011 UK Census</h2>
                    <p>Run by the Office for National Statistics. The smallest geographical area one can query is called an <b>Output Area</b></p>
                    <img src="assets/oa.png" style="margin:-10px;" />
                    <p></p>
				</section>

         <section data-background="assets/pres_bg.png">
					<h2>The 2011 UK Census</h2>
                    <p>I've queried my output area's "Religion by Age and Sex"</p>                 
                    <img src="assets/oadata1.png" style="border:none;" /></p>
                    <p  align='right'><small>My real output area's data!</small>

				</section>
                <section data-background="assets/pres_bg.png">
					<h2>My neighbour</h2>

                    <p>
                    <img src="assets/man.jpg" align="right" style="margin:20px;" />My neighbour is this man who's aged over 75.</p>
                    <p align='right'><small>My neighbour isn't really over 75, and this isn't really a photo of him.</small></p>
				</section>
                <section data-background="assets/pres_bg.png">
					<h2>Is it Private?</h2>
                    <img src="assets/oadata2.png" align="right" style="border:none; margin:20px;" width=25% />
                    <p>There's only one person (a man) over 75 in the data...</p>
                    <p class="fragment">It appears we could re-identify individuals.</p>
                    <p class="fragment">Although he's a very private person we can work out his religion by looking at the table.</p>
				</section>


                <section data-background="assets/pres_bg.png">
					<h2>Swapping</h2>                
                    <p>The ONS have thought of this and perform <b>targeted record swapping</b> to try and achieve anonymity.</p>
                    <p class="fragment">Rare households are more likely to be swapped</p>
                    <p class="fragment">Households are (usually) swapped within local authority areas</p>
                    <p class="fragment">Swaps are matched to preserve some statistical features</p>
				</section>

                <section data-background="assets/pres_bg.png">
					<h2>Summary</h2>   
                    <p>Making a dataset private needs more than just erasing names</p>             
                    <p class="fragment">To achieve a level of privacy the ONS have added <b>randomness</b> to the data</p>
                    <p class="fragment">This is a fundamental feature of differential privacy</p>
				</section>

                <section data-background="assets/pres_bg.png">
					<h2>Differential Privacy</h2>   
                    <p>Maximize the accuracy of queries from statistical databases while minimizing the chances of identifying its records.</p>
                    <p align='right'><small><a href="https://www.cis.upenn.edu/~aaroth/Papers/privacybook.pdf">Dwork and Roth. "The algorithmic foundations of differential privacy." </a></small></p>
				</section>


                <section data-background="assets/pres_bg.png">
					<h2>Example</h2>   
                    <p>We might want to find out whether people hold a controversial view, that they wouldn't normally express:</p> 
                    <h3>Is Machine Learning overrated?</h3>
                    <p>Many people in this room work in the field, but maybe wouldn't like to say 'Yes' to this question.</p>      

				</section>




                <section data-background="assets/pres_bg.png">					
                    <img src="assets/coinflip1.png"/>
				</section>

                <section data-background="assets/pres_bg.png">
                    <span><img src="assets/coinflip1.png" width=25% align="right" style="margin-bottom:100%"/>
If flips are different, tell the truth.</p> <p class="fragment">If they're the same...</p> <span class="fragment"><p>say</p><p>YES for heads</p><span class="fragment"><p>and</p><p> NO for tails</span>
				</section>


<section>
<small>
<table><tr><td></td><th>0</th><th>1</th><th>2</th><th>3</th><th>4</th><th>5</th><th>6</th><th>7</th><th>8</th><th>9</th><th>10</th><th>11</th><th>12</th><th>13</th><th>14</th><th>15</th><th>16</th><th>17</th><th>18</th><th>19</th></tr><tr><th>0</th><td>50</td><td>38</td><td>27</td><td>19</td><td>13</td><td>9</td><td>6</td><td>4</td><td>3</td><td>2</td><td>1</td><td>1</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>
<tr><th>1</th><td>63</td><td>50</td><td>38</td><td>28</td><td>20</td><td>14</td><td>10</td><td>7</td><td>4</td><td>3</td><td>2</td><td>1</td><td>1</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>
<tr><th>2</th><td>73</td><td>62</td><td>50</td><td>39</td><td>29</td><td>21</td><td>15</td><td>11</td><td>7</td><td>5</td><td>3</td><td>2</td><td>2</td><td>1</td><td>1</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>
<tr><th>3</th><td>81</td><td>72</td><td>62</td><td>50</td><td>40</td><td>30</td><td>22</td><td>16</td><td>11</td><td>8</td><td>6</td><td>4</td><td>3</td><td>2</td><td>1</td><td>1</td><td>1</td><td>0</td><td>0</td><td>0</td></tr>
<tr><th>4</th><td>87</td><td>80</td><td>72</td><td>61</td><td>50</td><td>40</td><td>31</td><td>23</td><td>17</td><td>12</td><td>9</td><td>6</td><td>4</td><td>3</td><td>2</td><td>1</td><td>1</td><td>1</td><td>0</td><td>0</td></tr>
<tr><th>5</th><td>91</td><td>86</td><td>79</td><td>71</td><td>61</td><td>51</td><td>41</td><td>32</td><td>24</td><td>18</td><td>13</td><td>10</td><td>7</td><td>5</td><td>3</td><td>2</td><td>2</td><td>1</td><td>1</td><td>1</td></tr>
<tr><th>6</th><td>94</td><td>91</td><td>85</td><td>79</td><td>70</td><td>61</td><td>51</td><td>41</td><td>32</td><td>25</td><td>19</td><td>14</td><td>10</td><td>8</td><td>5</td><td>4</td><td>3</td><td>2</td><td>1</td><td>1</td></tr>
<tr><th>7</th><td>96</td><td>94</td><td>90</td><td>85</td><td>78</td><td>69</td><td>60</td><td>51</td><td>42</td><td>33</td><td>26</td><td>20</td><td>15</td><td>11</td><td>8</td><td>6</td><td>4</td><td>3</td><td>2</td><td>2</td></tr>
<tr><th>8</th><td>97</td><td>96</td><td>93</td><td>89</td><td>84</td><td>77</td><td>69</td><td>60</td><td>51</td><td>42</td><td>34</td><td>27</td><td>21</td><td>16</td><td>12</td><td>9</td><td>7</td><td>5</td><td>3</td><td>3</td></tr>
<tr><th>9</th><td>98</td><td>97</td><td>95</td><td>92</td><td>88</td><td>83</td><td>76</td><td>68</td><td>60</td><td>51</td><td>42</td><td>35</td><td>28</td><td>22</td><td>17</td><td>13</td><td>10</td><td>7</td><td>5</td><td>4</td></tr>
<tr><th>10</th><td>99</td><td>98</td><td>97</td><td>95</td><td>92</td><td>87</td><td>82</td><td>75</td><td>68</td><td>59</td><td>51</td><td>43</td><td>35</td><td>28</td><td>23</td><td>18</td><td>14</td><td>10</td><td>8</td><td>6</td></tr>
<tr><th>11</th><td>99</td><td>99</td><td>98</td><td>96</td><td>94</td><td>91</td><td>87</td><td>81</td><td>75</td><td>67</td><td>59</td><td>51</td><td>43</td><td>36</td><td>29</td><td>23</td><td>18</td><td>14</td><td>11</td><td>8</td></tr>
<tr><th>12</th><td>99</td><td>99</td><td>98</td><td>97</td><td>96</td><td>94</td><td>90</td><td>86</td><td>80</td><td>74</td><td>67</td><td>59</td><td>51</td><td>43</td><td>36</td><td>30</td><td>24</td><td>19</td><td>15</td><td>12</td></tr>
<tr><th>13</th><td>100</td><td>99</td><td>99</td><td>98</td><td>97</td><td>95</td><td>93</td><td>90</td><td>85</td><td>80</td><td>73</td><td>66</td><td>59</td><td>51</td><td>44</td><td>37</td><td>30</td><td>25</td><td>20</td><td>16</td></tr>
<tr><th>14</th><td>100</td><td>100</td><td>99</td><td>99</td><td>98</td><td>97</td><td>95</td><td>92</td><td>89</td><td>84</td><td>79</td><td>73</td><td>66</td><td>58</td><td>51</td><td>44</td><td>37</td><td>31</td><td>25</td><td>21</td></tr>
<tr><th>15</th><td>100</td><td>100</td><td>100</td><td>99</td><td>99</td><td>98</td><td>96</td><td>94</td><td>92</td><td>88</td><td>84</td><td>78</td><td>72</td><td>65</td><td>58</td><td>51</td><td>44</td><td>38</td><td>32</td><td>26</td></tr>
<tr><th>16</th><td>100</td><td>100</td><td>100</td><td>99</td><td>99</td><td>98</td><td>97</td><td>96</td><td>94</td><td>91</td><td>87</td><td>83</td><td>78</td><td>71</td><td>65</td><td>58</td><td>51</td><td>44</td><td>38</td><td>32</td></tr>
<tr><th>17</th><td>100</td><td>100</td><td>100</td><td>100</td><td>99</td><td>99</td><td>98</td><td>97</td><td>96</td><td>93</td><td>91</td><td>87</td><td>82</td><td>77</td><td>71</td><td>65</td><td>58</td><td>51</td><td>45</td><td>38</td></tr>
<tr><th>18</th><td>100</td><td>100</td><td>100</td><td>100</td><td>100</td><td>99</td><td>99</td><td>98</td><td>97</td><td>95</td><td>93</td><td>90</td><td>86</td><td>82</td><td>76</td><td>71</td><td>64</td><td>58</td><td>51</td><td>45</td></tr>
<tr><th>19</th><td>100</td><td>100</td><td>100</td><td>100</td><td>100</td><td>99</td><td>99</td><td>99</td><td>98</td><td>97</td><td>95</td><td>92</td><td>89</td><td>86</td><td>81</td><td>76</td><td>70</td><td>64</td><td>58</td><td>51</td></tr>
</table>
</small>
</section>

<section data-background="assets/pres_bg.png">
<h2>Summary</h2>
<p>An individual's opinion is fairly safe in this example (plausible deniability)</p>
<p class="fragment">We can still perform inference over the whole group</p>
<p class="fragment">To protect the group, we could introduce more randomness</p>
</section>


<section data-background="assets/pres_bg_bb.png" data-markdown>
<script type="text/template">
<h2>Differential Privacy</h2>
<p>A randomised algorithm $M$ is $(\varepsilon, \delta)$-differentially private if, for all $m$ across its whole range, and for all neighbouring databases $D_1$ and $D_2$</p>
<p>$P\Big( M(D_1) = m \Big) \leq e^{\varepsilon} P\Big( M(D_2) = m \Big) + \delta$</p>
<p class="fragment">$\varepsilon$ puts a bound on how much privacy is lost by the query.</p>
<p class="fragment">Smaller $\varepsilon$ means more privacy (think about what happens if $\varepsilon=0$)</p>
<p class="fragment">$\delta$ says this inequality only holds with probability $1-\delta$</p>
</script>
</section>


<section data-background="assets/pres_bg_bb.png" data-markdown>
<script type="text/template">
<p>$P\Big( M(D_1) = m \Big) \leq e^{\varepsilon} P\Big( M(D_2) = m \Big) + \delta$</p>
<img src="assets/dp_graph.png" width=60%>
<p class="fragment">The ratio of probabilities between the outputs from two neighbouring databases can't be more than $e^\varepsilon$</p>

</script>
</section>


<section data-background="assets/pres_bg_bb.png" data-markdown>
<script type="text/template">
<p>For the coin-flip example, we want to compare the chance of a 'yes' response if the actual value is 'yes',</p>
<p>$P\Big(M(yes)=yes\Big)$<span class="fragment">$={}^3/_4$</span></p>
<p>with the chance of a 'yes' response if the actual value is 'no',</p>
<p>$P\Big(M(no)=yes\Big)$<span class="fragment">$={}^1/_4$</span></p>
<p class="fragment">Substituting in (ignoring $\delta$) $P\Big( M(D_1) = m \Big) \leq e^{\varepsilon} P\Big( M(D_2) = m \Big) + \delta$</p>
<p class="fragment">${}^3/_4 \leq e^{\varepsilon} \times {}^1/_4$</p>
<p class="fragment">$3 \leq e^{\varepsilon}$</p>
</script>
</section>


<section data-background="assets/pres_bg_bb.png" data-markdown>
<script type="text/template">
<p>The strongest bound, which is true for all neighbouring databases and output values is that</p>
<p>$\varepsilon = ln \; 3$</p>
<p>So we can write that:</p>
<br />
<p>The coin flip mechanism is $(ln\;3, 0)$-differentially private.</p>
</script>
</section>


<section data-background="assets/pres_bg_bb.png" data-markdown>
<script type="text/template">
<h2>Counting Queries</h2>
<p>How many people have diabetes?</p>
<img src="assets/db0.png">
<p class="fragment">We'll call this database $D_1$.</p>
<p class="fragment">Our query function, $M$, is summation.</p>
<p class="fragment">$M(D_1) = 2$</p>
</script>
</section>

<section data-background="assets/pres_bg_bb.png" data-markdown>
<script type="text/template">
<p>How many people have diabetes?</p>
<img src="assets/db.png">
<p class="fragment">We'll call this database $D_2$.</p>
<p class="fragment">$M(D_2) = 3$</p>
</script>
</section>

<section data-background="assets/pres_bg_bb.png" data-markdown>
<script type="text/template">
<p>Did the new person (Fred) have diabetes?</p>
<p class="fragment">$M(D_2)-M(D_1) = 3-2 = 1$</p>
<p class="fragment">Yes</p>
</script>
</section>

<section data-background="assets/pres_bg_bb.png" data-markdown>
<script type="text/template">
<p>
Differential Privacy works by considering the $l_1$ sensitivity of a function. That is; the magnitude of the change in its output one individual can cause.
</p>
<p class="fragment">In this case<br />$\Delta f= 1$</p>
</script>
</section>


<section data-background="assets/pres_bg_bb.png" data-markdown>
<script type="text/template">
<p>
To make our database queries differentially private we need to add noise to the output of the query, so $M(D) = f(D) + noise$.
<img src="assets/laplace1.png" style="border:none;"  />
</p>
</script>
</section>

<section data-background="assets/pres_bg_bb.png" data-markdown>
<script type="text/template">
<p>
We can see the effect of adding $\Delta f$ to the output.
<img src="assets/laplace2.png" style="border:none;" />
</p>
</script>
</section>


<section data-background="assets/pres_bg_bb.png" data-markdown>
<script type="text/template">
<p>
If the actual output value $m = 4$, what is the probability of that output, for the two databases?
<img src="assets/laplace3.png" style="border:none;" />
</p>
</script>
</section>


<section data-background="assets/pres_bg_bb.png" data-markdown>
<script type="text/template">
<img src="assets/laplace3.png" width=80%  style="border:none;"/>
<p>$M(D_2)$ was 3 times more likely to equal 4 than $M(D_1)$.</p>
</script>
</section>


<section data-background="assets/pres_bg_bb.png" data-markdown>
<script type="text/template">
<img src="assets/laplace3b.png" width=80% style="border:none;"/>
<p>$M(D_2)$ was also 3 times more likely to equal 3.5.</p>
</script>
</section>

<section data-background="assets/pres_bg_bb.png" data-markdown>
<script type="text/template">
<img src="assets/laplace3d.png" width=80% style="border:none;"/>
<p>$M(D_1)$ was 3 times more likely to equal 1.5 than $M(D_2)$.</p>
</script>
</section>

<section data-background="assets/pres_bg_bb.png" data-markdown>
<script type="text/template">
<img src="assets/laplace3e.png" width=80% style="border:none;"/>
<p>In between the two peaks, neither distribution is more than 3 times more likely than the other.</p>
</script>
</section>



<section data-background="assets/pres_bg_bb.png" data-markdown>
<script type="text/template">
<p>I've chosen the Laplace noise to have scale parameter, $b$, such that the privacy of this mechanism is equal to that of the coin flip example.</p>
<p class="fragment">For any pair of neighbouring databases,
$P\Big( M(D_2) = m \Big) \leq e^{ln\;3} \times P\Big( M(D_1) = m \Big)$</p>
<p class="fragment">
So we can say that (for the scale of the Laplace distribution I've chosen) this mechanism is $(ln \; 3,0)$-differentially private too. </p>
</script>
</section>

<section data-background="assets/pres_bg_bb.png" data-markdown>
<script type="text/template">
<img src="assets/epsilons.png" width=60% align="right" style="border:none; margin-left:50px;"/>
<h3>The Laplace Mechanism</h3>
<p>It's fairly easy to see that to have privacy $\varepsilon$, for the Laplace mechanism, you need scale parameter</p>
<p>$b = {{\Delta f} \over {b}}$</p>
</script>
</section>

<section data-background="assets/pres_bg_bb.png" data-markdown>
<script type="text/template">

<h3>Briefly: The $\delta$ parameter</h3>
<p>Consider adding uniform noise (with width=4).</p>
<img src="assets/delta1.png" width=50% style="border:none;"/>
<p>If you queried $D_1$ (without Fred), and received 0.5. You'd know there are between 0 and 2 people with diabetes.</p>
</script>
</section>

<section data-background="assets/pres_bg_bb.png" data-markdown>
<script type="text/template">
<img src="assets/delta2.png" width=50% style="border:none;"/>
<p>If you queried $D_2$ (with Fred), and received 4.5. You'd know there are between 3 and 6 people with diabetes.</p>
<p class="fragment">From these two queries you can be certain that Fred has diabetes. Although our randomised algorithm wouldn't always reveal Fred's status, it would do so with some probability, which is handled by the $\delta$.</p>
</script>
</section>

<section data-background="assets/pres_bg.png" data-markdown>
<script type="text/template">
<h2>Differential Privacy Summary</h2>
<p>This is a fairly new field for me. I'm starting to look at using its powerful privacy guarantees for the handling of data as part of the <a href="http://scikic.org">scikic</a> project.</p>
</script>
</section>


<section data-background="assets/pres_bg.png" data-markdown>
<script type="text/template">
<h2>What I'm doing with Differential Privacy</h2>
<p><a href="http://jmlr.org/papers/volume14/hall13a/hall13a.pdf">Hall et al. 2013</a> have developed a method for releasing <em>functions</em> while preserving privacy.</p>
<p>I'm applying their method to allow the release of a Gaussian Process's mean function.</p>
<p>Check back soon!</p>
</script>
</section>





<section data-background="assets/pres_bg.png">
<small>
<p align="left">
<span style='margin-left:-50px;'><b>The go-to book on differential privacy, by Dwork and Roth;</b><br/></span>
Dwork, Cynthia, and Aaron Roth. "The algorithmic foundations of differential privacy." Theoretical Computer Science 9.3-4 (2013): 211-407.
<a href="https://www.cis.upenn.edu/~aaroth/Papers/privacybook.pdf">link</a>
</p>
<p align="left">
<span style='margin-left:-50px;'><b>I found this paper allowed me to start applying DP to GP;</b><br/></span>
Hall, Rob, Alessandro Rinaldo, and Larry Wasserman. "Differential privacy for functions and functional data." The Journal of Machine Learning Research 14.1 (2013): 703-727.
<a href="http://www.stat.cmu.edu/~arinaldo/papers/hall13a.pdf">link</a>
</p>

<p align="left">
<span style='margin-left:-50px;'><b>Articles about the Massachusetts privacy debate</b><br/></span>
Barth-Jones, Daniel C. "The're-identification'of Governor William Weld's medical information: a critical re-examination of health data identification risks and privacy protections, then and now." Then and Now (June 4, 2012) (2012).  <a href="http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2076397">link</a>
</p>
<p align="left">
Ohm, Paul. "Broken promises of privacy: Responding to the surprising failure of anonymization." UCLA Law Review 57 (2010): 1701. <a href="https://epic.org/privacy/reidentification/ohm_article.pdf">link</a>
</p>

<p align="left">Narayanan, Arvind, and Edward W. Felten. "No silver bullet: De-identification still doesn’t work." White Paper (2014). <a href="http://randomwalker.info/publications/no-silver-bullet-de-identification.pdf">link</a></p>

<p align="left"><span style='margin-left:-50px;'><b>Images used:</b></span>
BostonGlobe: <a href="https://c.o0bg.com/rf/image_960w/Boston/2011-2020/2015/05/29/BostonGlobe.com/Business/Images/MassMutual_04.jpg">Mass Mutual</a>, 
 <a href="https://c.o0bg.com/rf/image_960w/Boston/2011-2020/2014/10/20/BostonGlobe.com/Metro/Images/Gov.%20Bill%20Weld%201-100425.jpg">Weld</a>. Harvard: <a href="http://www.gov.harvard.edu/files/Sweeney6crop.jpg">Sweeney</a>. Rich on flickr: <a href="https://www.flickr.com/photos/rich_b1982/13114665103/in/pool-sheffieldskyline/">Sheffield skyline</a>.</p>
<p>
<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
</p>
</small>
</section>


</div>

</div>

<script src="lib/js/head.min.js"></script>
<script src="js/reveal.js"></script>

<script>

	// Full list of configuration options available at:
	// https://github.com/hakimel/reveal.js#configuration
	Reveal.initialize({
		controls: true,
		progress: true,
		history: true,
		center: true,

        math: {
            mathjax: 'https://cdn.mathjax.org/mathjax/latest/MathJax.js',
            config: 'TeX-AMS_HTML-full'  // See http://docs.mathjax.org/en/latest/config-files.html
        },

		transition: 'slide', // none/fade/slide/convex/concave/zoom

		// Optional reveal.js plugins
		dependencies: [
			{ src: 'lib/js/classList.js', condition: function() { return !document.body.classList; } },
			{ src: 'plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
			{ src: 'plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
			{ src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
			{ src: 'plugin/zoom-js/zoom.js', async: true },
			{ src: 'plugin/notes/notes.js', async: true },
            { src: 'plugin/math/math.js', async: true }
		]
	});

</script>

</body>
</html>
